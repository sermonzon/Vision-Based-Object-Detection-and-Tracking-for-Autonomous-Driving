{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "851fca21-c5b0-42c9-b914-9b9513351fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from filterpy.kalman import KalmanFilter\n",
    "from ultralytics import YOLO\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77794c23-ca57-41ee-81b1-7a4e191df245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kalman Filter configuration\n",
    "dt = 1  # Time step\n",
    "kf_template = KalmanFilter(dim_x=8, dim_z=5)\n",
    "kf_template.F = np.array([\n",
    "    [1, 0, 0, 0, 0, dt,  0,  0],\n",
    "    [0, 1, 0, 0, 0,  0, dt,  0],\n",
    "    [0, 0, 1, 0, 0,  0,  0,  0],\n",
    "    [0, 0, 0, 1, 0,  0,  0,  0],\n",
    "    [0, 0, 0, 0, 1,  0,  0, dt],\n",
    "    [0, 0, 0, 0, 0,  1,  0,  0],\n",
    "    [0, 0, 0, 0, 0,  0,  1,  0],\n",
    "    [0, 0, 0, 0, 0,  0,  0,  1]\n",
    "])\n",
    "kf_template.H = np.array([\n",
    "    [1, 0, 0, 0, 0, 0, 0, 0],  # x\n",
    "    [0, 1, 0, 0, 0, 0, 0, 0],  # y\n",
    "    [0, 0, 1, 0, 0, 0, 0, 0],  # w\n",
    "    [0, 0, 0, 1, 0, 0, 0, 0],  # h\n",
    "    [0, 0, 0, 0, 1, 0, 0, 0]   # Z\n",
    "])\n",
    "kf_template.Q = np.diag([1, 1, 10, 10, 1, 1, 1, 10])\n",
    "kf_template.R = np.diag([10, 10, 100, 100, 10])\n",
    "\n",
    "# Tracking parameters\n",
    "tracked_objects = {}\n",
    "next_object_id = 0\n",
    "max_distance = 16\n",
    "max_frames_to_skip = 50\n",
    "min_confidence = 0.46\n",
    "confirmation_threshold = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8560d713-45ec-4e72-b3b1-b9fcf15595b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '34759_final_project_rect/seq_03/image_02/data/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m path_to_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m34759_final_project_rect/seq_03/image_02/data/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m path_to_right \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m34759_final_project_rect/seq_03/image_03/data/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 7\u001b[0m left_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_to_left\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      8\u001b[0m right_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(path_to_right))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(left_images) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(right_images), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLeft and right image sequences must have the same length.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '34759_final_project_rect/seq_03/image_02/data/'"
     ]
    }
   ],
   "source": [
    "# Load YOLO model\n",
    "model = YOLO('yolov8s.pt')\n",
    "\n",
    "# Load images\n",
    "path_to_left = '34759_final_project_rect/seq_03/image_02/data/'\n",
    "path_to_right = '34759_final_project_rect/seq_03/image_03/data/'\n",
    "left_images = sorted(os.listdir(path_to_left))\n",
    "right_images = sorted(os.listdir(path_to_right))\n",
    "\n",
    "assert len(left_images) == len(right_images), \"Left and right image sequences must have the same length.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2bc32d3-e68a-4e66-b10c-6f1d217166c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Camera parameters\n",
    "Camera_Left = np.array([[961.60052669, 0, 687.80277399],\n",
    "                        [0, 963.33029715, 235.14590179],\n",
    "                        [0, 0, 1]])\n",
    "Camera_Right = np.array([[906.92602542, 0, 683.78933836],\n",
    "                         [0, 909.61530614, 234.43994747],\n",
    "                         [0, 0, 1]])\n",
    "focal_length = Camera_Left[0, 0]\n",
    "baseline = 0.54\n",
    "\n",
    "# StereoBM object\n",
    "stereo = cv2.StereoSGBM_create(minDisparity=0,\n",
    "                                numDisparities=16 * 10,\n",
    "                                blockSize=15,\n",
    "                                P1=8 * 3 * 5 ** 2,\n",
    "                                P2=32 * 3 * 5 ** 2,\n",
    "                                disp12MaxDiff=1,\n",
    "                                uniquenessRatio=15,\n",
    "                                speckleWindowSize=50,\n",
    "                                speckleRange=2,\n",
    "                                preFilterCap=63,\n",
    "                                mode=cv2.STEREO_SGBM_MODE_SGBM_3WAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12c2504-21c6-44a5-8a2d-792eed885c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "total_overlap_ratio = 0\n",
    "frame_count = 0\n",
    "\n",
    "# Add class mapping for YOLO's class IDs to target categories\n",
    "class_map = {\n",
    "    0: \"Pedestrian\",\n",
    "    1: \"Cyclist\",\n",
    "    2: \"Car\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dec03f-054d-401b-9ae3-31fa89e067bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'left_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Main loop\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, (left_img_name, right_img_name) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[43mleft_images\u001b[49m, right_images)):\n\u001b[1;32m      3\u001b[0m     left_img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path_to_left, left_img_name)\n\u001b[1;32m      4\u001b[0m     right_img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path_to_right, right_img_name)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'left_images' is not defined"
     ]
    }
   ],
   "source": [
    "# Main loop\n",
    "for idx, (left_img_name, right_img_name) in enumerate(zip(left_images, right_images)):\n",
    "    left_img_path = os.path.join(path_to_left, left_img_name)\n",
    "    right_img_path = os.path.join(path_to_right, right_img_name)\n",
    "\n",
    "    img_left = cv2.imread(left_img_path)\n",
    "    img_right = cv2.imread(right_img_path)\n",
    "    \n",
    "    if img_left is None or img_right is None:\n",
    "        print(f\"Skipping invalid image pair: {left_img_path}, {right_img_path}\")\n",
    "        continue\n",
    "\n",
    "    # Compute disparity map\n",
    "    gray_left, gray_right = cv2.cvtColor(img_left, cv2.COLOR_BGR2GRAY), cv2.cvtColor(img_right, cv2.COLOR_BGR2GRAY)\n",
    "    disparity = stereo.compute(gray_left, gray_right).astype(np.float32) / 16.0\n",
    "    disparity[disparity <= 0] = np.nan\n",
    "\n",
    "    # Object detection\n",
    "    results = model(img_left)\n",
    "    boxes = results[0].boxes.xyxy.cpu().numpy()  # Bounding box coordinates\n",
    "    confidences = results[0].boxes.conf.cpu().numpy()  # Confidence scores\n",
    "    classes = results[0].boxes.cls.cpu().numpy()  # Class IDs\n",
    "\n",
    "    detections_list = []\n",
    "    for i in range(len(confidences)):\n",
    "        if confidences[i] >= min_confidence and int(classes[i]) in [0, 1, 2]:\n",
    "            x1, y1, x2, y2 = boxes[i]\n",
    "            width, height = x2 - x1, y2 - y1\n",
    "            x_center, y_center = x1 + width / 2, y1 + height / 2\n",
    "            disp = disparity[int(y_center), int(x_center)] if not np.isnan(disparity[int(y_center), int(x_center)]) else np.nan\n",
    "            if np.isnan(disp):\n",
    "                continue\n",
    "            Z = (focal_length * baseline) / disp\n",
    "\n",
    "            # Add detection to the list with mapped class name\n",
    "            detections_list.append({\n",
    "                'measurement': np.array([[x_center], [y_center], [width], [height], [Z]]),\n",
    "                'class_id': int(classes[i]),\n",
    "                'class_name': class_map.get(int(classes[i]), \"Unknown\")\n",
    "            })\n",
    "    \n",
    "    # Cyclist-Pedestrian Merging Logic\n",
    "    merged_indices = set()\n",
    "    \n",
    "    for i, cyclist in enumerate(detections_list):\n",
    "        if cyclist[\"class_name\"] == \"Cyclist\" and i not in merged_indices:\n",
    "            # Cyclist bounding box\n",
    "            x_c, y_c, w_c, h_c, z_c = cyclist['measurement'].flatten()\n",
    "            cyclist_bbox = [x_c - w_c / 2, y_c - h_c / 2, x_c + w_c / 2, y_c + h_c / 2]\n",
    "    \n",
    "            for j, pedestrian in enumerate(detections_list):\n",
    "                if pedestrian[\"class_name\"] == \"Pedestrian\" and j not in merged_indices:\n",
    "                    # Pedestrian bounding box\n",
    "                    x_p, y_p, w_p, h_p, _ = pedestrian['measurement'].flatten()\n",
    "                    pedestrian_bbox = [x_p - w_p / 2, y_p - h_p / 2, x_p + w_p / 2, y_p + h_p / 2]\n",
    "    \n",
    "                    # Check if the Pedestrian overlaps or is near the Cyclist\n",
    "                    horizontal_overlap = (cyclist_bbox[0] < pedestrian_bbox[2] and cyclist_bbox[2] > pedestrian_bbox[0])\n",
    "                    vertical_overlap = (cyclist_bbox[1] < pedestrian_bbox[3] and cyclist_bbox[3] > pedestrian_bbox[1])\n",
    "                    size_match = (w_c / 2) < w_p < (w_c * 2)\n",
    "    \n",
    "                    if horizontal_overlap and vertical_overlap and size_match:\n",
    "                        # Merge Pedestrian into Cyclist\n",
    "                        cyclist['measurement'][1] = min(cyclist['measurement'][1], pedestrian['measurement'][1])  # Update y_min\n",
    "                        cyclist['measurement'][3] = max(cyclist['measurement'][3], pedestrian['measurement'][3])  # Update y_max\n",
    "                        cyclist['measurement'][0] = min(cyclist['measurement'][0], pedestrian['measurement'][0])  # Update x_min\n",
    "                        cyclist['measurement'][2] = max(cyclist['measurement'][2], pedestrian['measurement'][2])  # Update x_max\n",
    "    \n",
    "                        merged_indices.add(j)  # Mark pedestrian as merged\n",
    "                        break\n",
    "    \n",
    "    # Remove all merged Pedestrians\n",
    "    detections_list = [det for idx, det in enumerate(detections_list) if idx not in merged_indices]\n",
    "    \n",
    "    # Remove overlaps after merging Cyclists and Pedestrians\n",
    "    final_detections = []\n",
    "    \n",
    "    for i, det in enumerate(detections_list):\n",
    "        if det[\"class_name\"] == \"Pedestrian\":\n",
    "            # Check if this Pedestrian overlaps with any Cyclist\n",
    "            is_overlapping = False\n",
    "            for cyclist in detections_list:\n",
    "                if cyclist[\"class_name\"] == \"Cyclist\":\n",
    "                    # Cyclist bounding box\n",
    "                    x_c, y_c, w_c, h_c, _ = cyclist['measurement'].flatten()\n",
    "                    cyclist_bbox = [x_c - w_c / 2, y_c - h_c / 2, x_c + w_c / 2, y_c + h_c / 2]\n",
    "    \n",
    "                    # Pedestrian bounding box\n",
    "                    x_p, y_p, w_p, h_p, _ = det['measurement'].flatten()\n",
    "                    pedestrian_bbox = [x_p - w_p / 2, y_p - h_p / 2, x_p + w_p / 2, y_p + h_p / 2]\n",
    "    \n",
    "                    # Check if Pedestrian overlaps Cyclist\n",
    "                    horizontal_overlap = (cyclist_bbox[0] < pedestrian_bbox[2] and cyclist_bbox[2] > pedestrian_bbox[0])\n",
    "                    vertical_overlap = (cyclist_bbox[1] < pedestrian_bbox[3] and cyclist_bbox[3] > pedestrian_bbox[1])\n",
    "    \n",
    "                    if horizontal_overlap and vertical_overlap:\n",
    "                        is_overlapping = True\n",
    "                        break\n",
    "    \n",
    "            # Keep the Pedestrian only if it does not overlap with any Cyclist\n",
    "            if not is_overlapping:\n",
    "                final_detections.append(det)\n",
    "        else:\n",
    "            # Keep all non-Pedestrian detections (e.g., Cyclists, Cars)\n",
    "            final_detections.append(det)\n",
    "    \n",
    "    detections_list = final_detections\n",
    "\n",
    "    # Kalman filter prediction step\n",
    "    for obj_id, obj in tracked_objects.items():\n",
    "        obj['kf'].predict()\n",
    "        obj['x_pred'], obj['P_pred'] = obj['kf'].x.copy(), obj['kf'].P.copy()\n",
    "\n",
    "    # Association via Linear Sum Assignment\n",
    "    track_ids, detection_ids = list(tracked_objects.keys()), list(range(len(detections_list)))\n",
    "    cost_matrix = np.zeros((len(tracked_objects), len(detections_list)))\n",
    "\n",
    "    for i, obj_id in enumerate(track_ids):\n",
    "        for j, detection in enumerate(detections_list):\n",
    "            kf = tracked_objects[obj_id]['kf']\n",
    "            innovation = detection['measurement'] - kf.H @ kf.x\n",
    "            innovation = innovation.reshape(-1, 1)\n",
    "            S = kf.H @ kf.P @ kf.H.T + kf.R\n",
    "            \n",
    "            # Compute the Mahalanobis distance\n",
    "            mahalanobis_distance = innovation.T @ np.linalg.inv(S) @ innovation\n",
    "    \n",
    "            # Ensure it is a scalar\n",
    "            cost_matrix[i, j] = np.sqrt(mahalanobis_distance.item())\n",
    "\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "\n",
    "    # Update matched tracks\n",
    "    matched_pairs = []\n",
    "    for i, j in zip(row_ind, col_ind):\n",
    "        if cost_matrix[i, j] < max_distance:\n",
    "            obj_id, detection = track_ids[i], detections_list[j]\n",
    "            tracked_objects[obj_id]['kf'].update(detection['measurement'])\n",
    "            tracked_objects[obj_id]['hit_streak'] += 1\n",
    "            tracked_objects[obj_id]['class_name'] = detection['class_name']  # Update class name\n",
    "            matched_pairs.append((obj_id, j))\n",
    "    \n",
    "    # Update unmatched tracks\n",
    "    for obj_id in set(track_ids) - set([x[0] for x in matched_pairs]):\n",
    "        tracked_objects[obj_id]['age'] += 1\n",
    "        if tracked_objects[obj_id]['age'] > max_frames_to_skip:\n",
    "            del tracked_objects[obj_id]\n",
    "\n",
    "    # Add new tracks for unmatched detections\n",
    "    for j in set(detection_ids) - set([x[1] for x in matched_pairs]):\n",
    "        kf = KalmanFilter(dim_x=8, dim_z=5)\n",
    "        kf.F, kf.H, kf.Q, kf.R = kf_template.F, kf_template.H, kf_template.Q, kf_template.R\n",
    "        detection = detections_list[j]\n",
    "        kf.x = np.vstack((detection['measurement'], np.zeros((3, 1))))\n",
    "        kf.P = np.diag([1000] * 8)\n",
    "        tracked_objects[next_object_id] = {\n",
    "            'kf': kf,\n",
    "            'hit_streak': 1,\n",
    "            'age': 0,\n",
    "            'class_id': detection['class_id'],\n",
    "            'class_name': detection['class_name']  # Store class name\n",
    "        }\n",
    "        next_object_id += 1\n",
    "\n",
    "    # Visualization\n",
    "    for obj_id, obj in tracked_objects.items():\n",
    "        if obj['hit_streak'] >= confirmation_threshold:\n",
    "            x, y, w, h = obj['kf'].x[:4, 0]\n",
    "            x1, y1, x2, y2 = int(x - w / 2), int(y - h / 2), int(x + w / 2), int(y + h / 2)\n",
    "            \n",
    "            if obj['class_name'] == \"Pedestrian\":\n",
    "                color = (0, 255, 0)  # Red for Pedestrians\n",
    "            elif obj['class_name'] == \"Cyclist\":\n",
    "                color = (0, 0, 255)  # Green for Cyclists\n",
    "            elif obj['class_name'] == \"Car\":\n",
    "                color = (255, 0, 0)  # Blue for Cars\n",
    "            else:\n",
    "                color = (255, 255, 255)  # White for Unknown (if any)\n",
    "    \n",
    "            cv2.rectangle(img_left, (x1, y1), (x2, y2), color, 2)\n",
    "            \n",
    "            label = f\"ID {obj_id} | {obj['class_name']} | Z: {obj['kf'].x[4, 0]:.2f}\"\n",
    "            cv2.putText(img_left, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    \n",
    "    cv2.imshow('Tracking', img_left)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81dc879-0163-4f7d-a942-bd91eab67812",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
